{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:04:14.731012Z",
     "start_time": "2023-04-25T11:04:09.671301Z"
    }
   },
   "outputs": [],
   "source": [
    "from Indexed_Dataset import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:04:14.785832Z",
     "start_time": "2023-04-25T11:04:14.731012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9a7802ded0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:04:15.775358Z",
     "start_time": "2023-04-25T11:04:14.785832Z"
    }
   },
   "outputs": [],
   "source": [
    "train_arr = np.loadtxt(\"../data/combined_trainvf_trig.csv\", dtype=np.float32, delimiter=\",\", skiprows=1)\n",
    "mean = train_arr[:, -1].mean()\n",
    "std = train_arr[:, -1].std()\n",
    "# train_arr = train_arr[train_arr[:, -1] < mean + 5 * std]\n",
    "# train_arr = train_arr[train_arr[:, -1] > 50]\n",
    "# Split into training/validation sets\n",
    "np.random.shuffle(train_arr)\n",
    "valid_arr = train_arr[int(0.9 * train_arr.shape[0]):]\n",
    "train_arr = train_arr[:int(0.9 * train_arr.shape[0])]\n",
    "weights = train_arr[:, -1]\n",
    "\n",
    "# sampler = WeightedRandomSampler(weights=weights, num_samples=train_arr.shape[0], replacement=True)\n",
    "\n",
    "train_set = Indexed_Dataset(arr=train_arr)\n",
    "train_load = DataLoader(dataset=train_set,\n",
    "                        batch_size=32,\n",
    "#                         sampler=sampler,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        worker_init_fn=seed_worker,\n",
    "                        generator=g,)\n",
    "\n",
    "valid_set = Indexed_Dataset(arr=valid_arr)\n",
    "valid_load = DataLoader(dataset=valid_set,\n",
    "                        batch_size=32,\n",
    "                        # num_workers=8,\n",
    "                        shuffle=True,\n",
    "                        worker_init_fn=seed_worker,\n",
    "                        generator=g,)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:06:37.789948Z",
     "start_time": "2023-04-25T11:06:37.758708Z"
    }
   },
   "outputs": [],
   "source": [
    "class TravelRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        origin_call_dim = 20\n",
    "        origin_stand_dim = 5\n",
    "        taxi_id_dim = 10\n",
    "        self.embed_origin_call: nn.Module = nn.Embedding(29027, origin_call_dim, padding_idx=0)\n",
    "        self.embed_origin_stand: nn.Module = nn.Embedding(64, origin_stand_dim)\n",
    "        self.embed_taxi_id: nn.Module = nn.Embedding(448, taxi_id_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(22 + origin_call_dim + origin_stand_dim + taxi_id_dim, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        origin_call = self.embed_origin_call(input[:, 0].to(dtype=torch.int32))\n",
    "        origin_stand = self.embed_origin_stand(input[:, 1].to(dtype=torch.int32))\n",
    "        taxi_id = self.embed_taxi_id(input[:, 2].to(dtype=torch.int32))\n",
    "        input = torch.cat((origin_call, origin_stand, taxi_id, input[:, 3:]), dim=1).to(dtype=torch.float32)\n",
    "        input = self.feed_forward(input)\n",
    "        return input\n",
    "\n",
    "model = TravelRegressor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:06:39.219554Z",
     "start_time": "2023-04-25T11:06:39.190938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(train_set[0][0].unsqueeze(0).to(device)).size()\n",
    "# train_set[0][0].unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([315.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T14:02:31.352229Z",
     "start_time": "2023-04-16T14:02:31.304302Z"
    }
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_mse(pred, act):\n",
    "    denom = torch.sum(act)\n",
    "    return torch.sum(act * ((pred - act)**2 / len(act))) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T14:02:31.357059Z",
     "start_time": "2023-04-16T14:02:31.335915Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    loss = torch.tensor([0]).to(device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        for i, (entry, target) in enumerate(valid_load):\n",
    "            entry = entry.to(device, dtype=torch.float32)\n",
    "            target = target.to(device, dtype=torch.float32)\n",
    "            preds = model(entry)\n",
    "            loss += mse(preds, target)\n",
    "\n",
    "    return loss /(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T14:02:31.405736Z",
     "start_time": "2023-04-16T14:02:31.362017Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_loss = []\n",
    "valid_loss = []\n",
    "def train(num_iter: int):\n",
    "    for epoch in range(num_iter):\n",
    "        rolling_loss = 0\n",
    "        entries = 0\n",
    "        for i, (entry, target) in enumerate(train_load):\n",
    "            model.train()\n",
    "            entry = entry.to(device, dtype=torch.float32)\n",
    "            target = target.to(device, dtype=torch.float32)\n",
    "            preds = model(entry)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = mse(preds, target) # or weighted mse\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rolling_loss += mse(preds, target) * entry.size(0)\n",
    "            entries += entry.size(0)\n",
    "            if (i + 1) % 500 == 0 or i == 0:\n",
    "                rolling_loss /= entries\n",
    "                print(f\"[Epoch: {epoch + 1}]\\t[Iter: {i + 1}]\\t[RMSE: {torch.sqrt(rolling_loss)}]\\t[STD: {torch.std(preds).item()}]\")\n",
    "                rmse_loss.append(torch.sqrt(rolling_loss).item())\n",
    "#                 valid_loss.append(torch.sqrt(validate()).item())\n",
    "                entries = 0\n",
    "                rolling_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1]\t[Iter: 1]\t[RMSE: 415.27630615234375]\t[STD: 4.039961338043213]\n",
      "[Epoch: 1]\t[Iter: 500]\t[RMSE: 698.8008422851562]\t[STD: 10.465325355529785]\n",
      "[Epoch: 1]\t[Iter: 1000]\t[RMSE: 673.4603881835938]\t[STD: 19.266572952270508]\n",
      "[Epoch: 1]\t[Iter: 1500]\t[RMSE: 689.7603149414062]\t[STD: 41.54059600830078]\n",
      "[Epoch: 1]\t[Iter: 2000]\t[RMSE: 706.648681640625]\t[STD: 64.16667175292969]\n",
      "[Epoch: 1]\t[Iter: 2500]\t[RMSE: 706.2863159179688]\t[STD: 68.70936584472656]\n",
      "[Epoch: 1]\t[Iter: 3000]\t[RMSE: 660.0432739257812]\t[STD: 80.0284194946289]\n",
      "[Epoch: 1]\t[Iter: 3500]\t[RMSE: 698.544677734375]\t[STD: 126.25977325439453]\n",
      "[Epoch: 1]\t[Iter: 4000]\t[RMSE: 660.4996948242188]\t[STD: 93.74070739746094]\n",
      "[Epoch: 1]\t[Iter: 4500]\t[RMSE: 618.5896606445312]\t[STD: 145.86888122558594]\n",
      "[Epoch: 1]\t[Iter: 5000]\t[RMSE: 710.1400756835938]\t[STD: 104.73110961914062]\n",
      "[Epoch: 1]\t[Iter: 5500]\t[RMSE: 570.0415649414062]\t[STD: 96.78145599365234]\n",
      "[Epoch: 1]\t[Iter: 6000]\t[RMSE: 752.9943237304688]\t[STD: 203.1099853515625]\n",
      "[Epoch: 1]\t[Iter: 6500]\t[RMSE: 616.9595947265625]\t[STD: 110.81607055664062]\n",
      "[Epoch: 1]\t[Iter: 7000]\t[RMSE: 689.5797729492188]\t[STD: 123.98170471191406]\n",
      "[Epoch: 1]\t[Iter: 7500]\t[RMSE: 668.0501708984375]\t[STD: 143.04449462890625]\n",
      "[Epoch: 1]\t[Iter: 8000]\t[RMSE: 668.8515014648438]\t[STD: 178.82781982421875]\n",
      "[Epoch: 1]\t[Iter: 8500]\t[RMSE: 669.2947998046875]\t[STD: 121.70487213134766]\n",
      "[Epoch: 1]\t[Iter: 9000]\t[RMSE: 631.156494140625]\t[STD: 134.83505249023438]\n",
      "[Epoch: 1]\t[Iter: 9500]\t[RMSE: 725.0726928710938]\t[STD: 153.0651397705078]\n",
      "[Epoch: 1]\t[Iter: 10000]\t[RMSE: 594.8177490234375]\t[STD: 127.12212371826172]\n",
      "[Epoch: 1]\t[Iter: 10500]\t[RMSE: 650.6309204101562]\t[STD: 127.89633178710938]\n",
      "[Epoch: 1]\t[Iter: 11000]\t[RMSE: 713.9102783203125]\t[STD: 163.1834259033203]\n",
      "[Epoch: 1]\t[Iter: 11500]\t[RMSE: 660.5499877929688]\t[STD: 129.60836791992188]\n",
      "[Epoch: 1]\t[Iter: 12000]\t[RMSE: 616.2786254882812]\t[STD: 195.25445556640625]\n",
      "[Epoch: 1]\t[Iter: 12500]\t[RMSE: 676.75927734375]\t[STD: 251.57110595703125]\n",
      "[Epoch: 1]\t[Iter: 13000]\t[RMSE: 666.855224609375]\t[STD: 125.00051879882812]\n",
      "[Epoch: 1]\t[Iter: 13500]\t[RMSE: 664.5945434570312]\t[STD: 222.828857421875]\n",
      "[Epoch: 1]\t[Iter: 14000]\t[RMSE: 639.05126953125]\t[STD: 72.30294036865234]\n",
      "[Epoch: 1]\t[Iter: 14500]\t[RMSE: 736.2088012695312]\t[STD: 109.71125030517578]\n",
      "[Epoch: 1]\t[Iter: 15000]\t[RMSE: 621.124267578125]\t[STD: 158.99256896972656]\n",
      "[Epoch: 1]\t[Iter: 15500]\t[RMSE: 573.4246215820312]\t[STD: 124.8515396118164]\n",
      "[Epoch: 1]\t[Iter: 16000]\t[RMSE: 717.02197265625]\t[STD: 172.87985229492188]\n",
      "[Epoch: 1]\t[Iter: 16500]\t[RMSE: 632.7487182617188]\t[STD: 195.58335876464844]\n",
      "[Epoch: 1]\t[Iter: 17000]\t[RMSE: 659.016845703125]\t[STD: 88.1557846069336]\n",
      "[Epoch: 1]\t[Iter: 17500]\t[RMSE: 668.48583984375]\t[STD: 170.55870056152344]\n",
      "[Epoch: 1]\t[Iter: 18000]\t[RMSE: 617.66748046875]\t[STD: 234.72828674316406]\n",
      "[Epoch: 1]\t[Iter: 18500]\t[RMSE: 693.3744506835938]\t[STD: 147.59417724609375]\n",
      "[Epoch: 1]\t[Iter: 19000]\t[RMSE: 646.3427124023438]\t[STD: 136.1182098388672]\n",
      "[Epoch: 1]\t[Iter: 19500]\t[RMSE: 584.8231201171875]\t[STD: 160.69735717773438]\n",
      "[Epoch: 1]\t[Iter: 20000]\t[RMSE: 646.65625]\t[STD: 112.97926330566406]\n",
      "[Epoch: 1]\t[Iter: 20500]\t[RMSE: 696.31396484375]\t[STD: 105.24564361572266]\n",
      "[Epoch: 1]\t[Iter: 21000]\t[RMSE: 670.207275390625]\t[STD: 141.1669158935547]\n",
      "[Epoch: 1]\t[Iter: 21500]\t[RMSE: 747.507568359375]\t[STD: 118.98452758789062]\n",
      "[Epoch: 1]\t[Iter: 22000]\t[RMSE: 674.8135375976562]\t[STD: 163.04852294921875]\n",
      "[Epoch: 1]\t[Iter: 22500]\t[RMSE: 567.3084716796875]\t[STD: 140.3243865966797]\n",
      "[Epoch: 1]\t[Iter: 23000]\t[RMSE: 688.4140014648438]\t[STD: 201.8477783203125]\n",
      "[Epoch: 1]\t[Iter: 23500]\t[RMSE: 676.8392333984375]\t[STD: 135.61318969726562]\n",
      "[Epoch: 1]\t[Iter: 24000]\t[RMSE: 630.2039794921875]\t[STD: 112.93896484375]\n",
      "[Epoch: 1]\t[Iter: 24500]\t[RMSE: 593.3665161132812]\t[STD: 503.6806640625]\n",
      "[Epoch: 1]\t[Iter: 25000]\t[RMSE: 646.1344604492188]\t[STD: 128.40516662597656]\n",
      "[Epoch: 1]\t[Iter: 25500]\t[RMSE: 655.1808471679688]\t[STD: 149.8638916015625]\n",
      "[Epoch: 1]\t[Iter: 26000]\t[RMSE: 591.6835327148438]\t[STD: 139.48574829101562]\n",
      "[Epoch: 1]\t[Iter: 26500]\t[RMSE: 607.4634399414062]\t[STD: 216.91807556152344]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14506/2140545206.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_14506/3626768534.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_iter)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# or weighted mse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mrolling_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(rmse_loss, label=\"training\")\n",
    "plt.plot(valid_loss, label=\"validation\")\n",
    "plt.xlabel(\"Iteration's\")\n",
    "plt.ylabel(\"RMSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(validate()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone part 3 getting top 10 data points with greatest training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tensor = torch.from_numpy(train_arr).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# import gc\n",
    "# # del variables\n",
    "# gc.collect()\n",
    "# # Ran into CUDA out of memory :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_load = DataLoader(dataset=train_set,\n",
    "#                         batch_size=32,\n",
    "# #                         sampler=sampler,\n",
    "#                         shuffle=False, # I just disabled shuffling\n",
    "#                         num_workers=8,\n",
    "#                         worker_init_fn=seed_worker,\n",
    "#                         generator=g,)\n",
    "# model.eval()\n",
    "# full_loss = np.zeros(len(train_set))\n",
    "# total = 0\n",
    "# batchsz = 0\n",
    "# with torch.no_grad():\n",
    "#     for i, (entry, target) in enumerate(eval_load):\n",
    "#         entry = entry.to(device, dtype=torch.float32)\n",
    "#         target = target.to(device, dtype=torch.float32)\n",
    "#         preds = model(entry)\n",
    "#         batchsz = target.size()[0]\n",
    "#         full_loss[total:total + batchsz] = np.sqrt((preds.cpu().numpy() - target.cpu().numpy())**2)[:, 0]\n",
    "#         total += target.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = np.argsort(full_loss)\n",
    "# print(order[-10:]) # 10 worst loss data points\n",
    "# print(full_loss[order[-10:]])\n",
    "# print(train_arr[order[-10:], 0]) # indices in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(train_arr[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_arr[train_arr[:, 0].astype(np.int32), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_arr = np.loadtxt(\"../data/no_coord_train.csv\", dtype=np.float32, delimiter=\",\", skiprows=1)\n",
    "# eval_set = Indexed_Dataset(arr=eval_arr)\n",
    "# eval_load = DataLoader(dataset=eval_set,\n",
    "#                         batch_size=32,\n",
    "# #                         sampler=sampler,\n",
    "#                         shuffle=False, # I just disabled shuffling\n",
    "#                         num_workers=8,\n",
    "#                         worker_init_fn=seed_worker,\n",
    "#                         generator=g,)\n",
    "# model.eval()\n",
    "# full_loss = np.zeros(len(eval_set))\n",
    "# total = 0\n",
    "# batchsz = 0\n",
    "# with torch.no_grad():\n",
    "#     for i, (entry, target) in enumerate(eval_load):\n",
    "#         entry = entry.to(device, dtype=torch.float32)\n",
    "#         target = target.to(device, dtype=torch.float32)\n",
    "#         preds = model(entry)\n",
    "#         batchsz = target.size()[0]\n",
    "#         full_loss[total:total + batchsz] = np.sqrt((preds.cpu().numpy() - target.cpu().numpy())**2)[:, 0]\n",
    "#         total += target.size()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = np.argsort(full_loss)\n",
    "# print(len(order))\n",
    "# print(full_loss[order[-10:]])\n",
    "# print(order[-10:]) # 10 worst loss data points\n",
    "# eval_arr[order[-10:], 0] # indices in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test = torch.from_numpy(np.loadtxt(\"../data/no_coord_test.csv\", skiprows=1, dtype=np.float32, delimiter=\",\")).to(device)\n",
    "#     out = model(test)\n",
    "#     torch.set_printoptions(threshold=10000, sci_mode=False)\n",
    "#     df_pred = pd.read_csv(\"../data/sampleSubmission.csv\")\n",
    "#     df_pred[\"TRAVEL_TIME\"] = out.cpu().numpy()\n",
    "#     df_pred.to_csv(\"NN_nocoord_weighted_prune.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
